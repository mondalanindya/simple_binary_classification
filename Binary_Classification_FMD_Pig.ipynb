{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mondalanindya/simple_binary_classification/blob/main/Binary_Classification_FMD_Pig.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMD Pig Dataset\n",
        "\n",
        "The [FMD Pig](https://zenodo.org/record/7778284/files/FMD_Pig.zip) is a collection of pig images with and without foot and mouth diseases. So it's a dataset with two classes.\n",
        "\n",
        "In this project, we will implement a convolutional neural network (CNN) model for classifying the images of the FMD Pig dataset. We will use PyTorch deep learning framework to complete our task."
      ],
      "metadata": {
        "id": "2F3kivrdltBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists('FMD_Pig.zip'):\n",
        "    !wget https://zenodo.org/record/7778284/files/FMD_Pig.zip\n",
        "    !unzip -q FMD_Pig.zip\n",
        "    !rm FMD_Pig.zip"
      ],
      "metadata": {
        "id": "nGq4Fk3PZWxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset and DataLoader"
      ],
      "metadata": {
        "id": "iEAsP5FRfTSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, we will be defining datasets and data loaders necessary for our training. Details on datasets and dataloaders can be found in the [documentation](https://pytorch.org/vision/stable/datasets.html)."
      ],
      "metadata": {
        "id": "0nFCiLJOh3Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "\n",
        "# Before defining datasets, lets define how images should be transformed. This is \n",
        "# because the transformations should go with the definitions of datasets. In this\n",
        "# tutorial we will using simple transformations, such as (1) image to tensor, (2)\n",
        "# normalization.\n",
        "image_transform = Compose([Resize((112, 112)), ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Once we have the transformations defined, lets define the dataset\n",
        "dataset = ImageFolder('FMD_Pig', transform=image_transform)\n",
        "\n",
        "# Once we have the dataset, lets split it into train and test set in 80% and 20%\n",
        "# split\n",
        "train_dataset, test_dataset = random_split(dataset=dataset, lengths=[0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# Once we have the datasets defined, lets define the data loaders as follows\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "R8N2vhP8a3Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdYlqJ7xDvJa",
        "pycharm": {}
      },
      "source": [
        "### Example Image\n",
        "Lets have a look on how images from FMD_Pig dataset looks like. Since the images are already normalized, their resolutions might have slightly changed. To visualize the original images, we should have ideally apply a reverse transformation which is avoided to keep this tutorial simple and brief."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4FrTxdxDwnE",
        "pycharm": {}
      },
      "source": [
        "# import plot library\n",
        "import matplotlib.pyplot as plt\n",
        "# iterate the dataloader\n",
        "_, (example_datas, labels) = next(enumerate(train_loader))\n",
        "# get the first data\n",
        "sample = example_datas[0]\n",
        "# show the data\n",
        "plt.imshow(sample.permute(1, 2, 0))\n",
        "print(\"Label: \" + str(labels[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUWGhlsMCYZD",
        "pycharm": {}
      },
      "source": [
        "## Model\n",
        "Now, we have to define trainable layers with parameters and put them inside a model. Have a look on the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module) of `nn.Module` and read more about different layers and functionalities of PyTorch there. Here we are going to implement various versions of AlexNet model and use it for classification. In this model, we are going to use the following functions or modules:\n",
        "\n",
        "* `nn.Conv2d()`: It is a PyTorch module that applies a 2D convolution over an input signal composed of several input planes. More details are available on the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
        "\n",
        "* `nn.Linear()`: It is a module that applies a linear transformation to the incoming data. More details can be found in its [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear).\n",
        "\n",
        "* `nn.ReLU()`: It is also a module that applies element-wise the rectified linear unit function. Its [documentation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#relu) can explain more.\n",
        "\n",
        "* `nn.Dropout()`: This module randomly zeroes some of the elements of the input tensor with probability `p`. Check the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can define a model in several ways. Below, we show some of them."
      ],
      "metadata": {
        "id": "yjGEol-IX31p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMtPp5OeCakG",
        "pycharm": {}
      },
      "source": [
        "## We first import the pytorch nn module and optimizer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "## Below we define the model as defined in the torchvision package and initialised \n",
        "## with pretrained weights (see pretrained=True flag)\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(AlexNet, self).__init__()\n",
        "        from torchvision import models\n",
        "        alexnet = models.alexnet(weights='IMAGENET1K_V1')\n",
        "        self.features = alexnet.features\n",
        "        self.avgpool = alexnet.avgpool\n",
        "        self.classifier = alexnet.classifier\n",
        "        # Please note how to change the last layer of the classifier for a new dataset\n",
        "        # ImageNet-1K has 1000 classes, but STL-10 has 10 classes\n",
        "        self.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        # Note how we are flattening the feature map, B x C x H x W -> B x C*H*W\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nayicPkJCkWy",
        "pycharm": {}
      },
      "source": [
        "## Initialization\n",
        "Once we have the model defined, lets instantiate it and set other hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model\n",
        "We will initialize the model, transfer to the desired device and set the parameters to receive gradients."
      ],
      "metadata": {
        "id": "Y6YBtqhvZGwG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjyEGZSdCk_i",
        "pycharm": {}
      },
      "source": [
        "# define the model, we use AlexNet\n",
        "model = AlexNet(2) # since FMD Pig dataset has 2 classes, we set num_classes = 2\n",
        "# device: cuda (gpu) or cpu\n",
        "device = \"cuda\"\n",
        "# map to device\n",
        "model = model.to(device) # `model.cuda()` will also do the same job\n",
        "# make the parameters trainable\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizer\n",
        "For updating the parameters, PyTorch provides the package torch.optim that has most popular optimizers implemented. In this tutorial, we will be using the `torch.optim.Adam` optimizer.\n"
      ],
      "metadata": {
        "id": "CETvGvW8Y5-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "## some hyperparameters related to optimizer\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 0.0005\n",
        "# define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "jEBBRoh-Y-bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "-RkqO0VcZuRP"
      },
      "source": [
        "## Average Meter\n",
        "It is a simple class for keeping training statistics, such as losses and accuracies etc. The `.val` field usually holds the statistics for the current batch, whereas the `.avg` field hold statistics for the current epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeLH7fbOHDhH",
        "pycharm": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSk4VfO_C4tL",
        "pycharm": {}
      },
      "source": [
        "## Train and Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJeHMF_BC7bg",
        "pycharm": {}
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "##define train function\n",
        "def train(model, device, train_loader, optimizer):\n",
        "    # meter\n",
        "    loss = AverageMeter()\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
        "    for batch_idx, (data, target) in enumerate(tk0):\n",
        "        # after fetching the data transfer the model to the \n",
        "        # required device, in this example the device is gpu\n",
        "        # transfer to gpu can also be done by \n",
        "        # data, target = data.cuda(), target.cuda()\n",
        "        data, target = data.to(device), target.to(device)  \n",
        "        # compute the forward pass\n",
        "        # it can also be achieved by model.forward(data)\n",
        "        output = model(data) \n",
        "        # compute the loss function\n",
        "        loss_this = F.cross_entropy(output, target)\n",
        "        # initialize the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # compute the backward pass\n",
        "        loss_this.backward()\n",
        "        # update the parameters\n",
        "        optimizer.step()\n",
        "        # update the loss meter \n",
        "        loss.update(loss_this.item(), target.shape[0])\n",
        "    print('Train: Average loss: {:.4f}\\n'.format(loss.avg))\n",
        "    return loss.avg\n",
        "        \n",
        "##define test function\n",
        "def test(model, device, test_loader):\n",
        "    # meters\n",
        "    loss = AverageMeter()\n",
        "    acc = AverageMeter()\n",
        "    correct = 0\n",
        "    # switch to test mode\n",
        "    model.eval()\n",
        "    for data, target in test_loader:\n",
        "        # after fetching the data transfer the model to the \n",
        "        # required device, in this example the device is gpu\n",
        "        # transfer to gpu can also be done by \n",
        "        # data, target = data.cuda(), target.cuda()\n",
        "        data, target = data.to(device), target.to(device)  # data, target = data.cuda(), target.cuda()\n",
        "        # since we dont need to backpropagate loss in testing,\n",
        "        # we dont keep the gradient\n",
        "        with torch.no_grad():\n",
        "            # compute the forward pass\n",
        "            # it can also be achieved by model.forward(data)\n",
        "            output = model(data)\n",
        "        # compute the loss function just for checking\n",
        "        loss_this = F.cross_entropy(output, target) # sum up batch loss\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.argmax(dim=1, keepdim=True) \n",
        "        # check which of the predictions are correct\n",
        "        correct_this = pred.eq(target.view_as(pred)).sum().item()\n",
        "        # accumulate the correct ones\n",
        "        correct += correct_this\n",
        "        # compute accuracy\n",
        "        acc_this = correct_this/target.shape[0]*100.0\n",
        "        # update the loss and accuracy meter \n",
        "        acc.update(acc_this, target.shape[0])\n",
        "        loss.update(loss_this.item(), target.shape[0])\n",
        "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        loss.avg, correct, len(test_loader.dataset), acc.avg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzxivyrXDB7a",
        "pycharm": {}
      },
      "source": [
        "## Training Loop\n",
        "Training loop containing alternating train and test phase. Below we are iterating the loops 5 times, you can iterate more times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tna_R8TSDD4D",
        "pycharm": {
          "is_executing": true
        }
      },
      "source": [
        "# number of epochs we decide to train\n",
        "num_epoch = 5\n",
        "for epoch in range(1, num_epoch + 1):\n",
        "    epoch_loss = train(model, device, train_loader, optimizer)\n",
        "test(model, device, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B19bDzBDIV4",
        "pycharm": {}
      },
      "source": [
        "### Summary\n",
        "Show the summary of the model. It shows the number of parameters in layerwise as well as the total number of parameters. It also shows the memories required for training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eztTJ4VDKCA",
        "pycharm": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(model, (3, 96, 96))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}